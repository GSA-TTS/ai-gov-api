# Test Cases for Zero Trust: Continuous Monitoring Verification

This document outlines test cases for verifying Continuous Monitoring and logging mechanisms of the GSAi API Framework, based on Zero Trust testing strategies detailed in Section 7.12 of the TestPlan.md.

**Test Cases Summary: 16 (Original: 8, Enhanced: +8)**

**Referenced Code Components:**
* **Request Correlation:** app/logs/middleware.py:11-47 StructlogMiddleware implementing comprehensive request tracking with UUID generation and context binding
* **Logging Infrastructure:** app/logs/logging_config.py:11-48 structured logging configuration with JSON formatting for production environments
* **Context Management:** app/logs/logging_context.py provides request_id propagation across service boundaries
* **Security Event Logging:** app/auth/dependencies.py:32-43 logs authentication failures and API key access patterns
* **Usage Auditing:** app/services/billing.py:13-23 logs LLM usage for compliance and audit trails
* **Provider Interaction Logging:** app/providers/vertex_ai/vertexai.py:80-99-116 captures model metrics and LLM interaction details
* **Missing PIIFilteringProcessor:** Referenced in original document but not implemented in app/logs/logging_config.py:1-54

### **ZTA_CM_001**
* **ID:** ZTA_CM_001
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Verify comprehensive request tracking with UUID and context binding.
* **Exposure Point(s):** `StructlogMiddleware` (`app/logs/middleware.py:11-47`), `app/logs/logging_context.py`.
* **Test Method/Action:** Make an API call (e.g., GET `/api/v1/models`). Inspect server logs.
* **Prerequisites:** API is running with logging configured.
* **Expected Secure Outcome:** Every request is assigned a unique `request_id`. This ID, along with other context like client IP and user agent, is logged for both the start and end of the request, enabling comprehensive audit trails.
* **Verification Steps:**
    1.  Make an API request.
    2.  Find the log entries for this request in the server logs.
    3.  Confirm a unique `request_id` is present in the log entries related to this request.
    4.  Confirm fields like `client_ip`, `method`, `path`, `user_agent`, `status_code`, `duration_ms` are logged.

### **ZTA_CM_002**
* **ID:** ZTA_CM_002
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Verify structured logging configuration for production environments.
* **Exposure Point(s):** `app/logs/logging_config.py:11-48`.
* **Test Method/Action:** Review logging configuration and inspect log output in a production-like setting (where `ENV` is not "dev").
* **Prerequisites:** API is running. `ENV` environment variable set to "prod" or similar.
* **Expected Secure Outcome:** Logs are formatted in JSON for production environments, facilitating machine parsing and ingestion into log management/SIEM systems. Timestamps and log levels are correctly managed.
* **Verification Steps:**
    1.  Ensure `LOG_FORMATTER` in `app/config/settings.py` is "json" or that `app/logs/logging_config.py` defaults to JSONRenderer when `ENV` is not "dev".
    2.  Inspect raw log output. Confirm it is valid JSON.
    3.  Verify each JSON log entry contains standard fields like `event` (log message), `level` (e.g., "info", "error"), `timestamp`, and the contextual data from Structlog.

### **ZTA_CM_003**
* **ID:** ZTA_CM_003
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Verify logging of authentication failures.
* **Exposure Point(s):** `valid_api_key` function in `app/auth/dependencies.py:32-43` (specifically, the `logger.warning` calls).
* **Test Method/Action:** Attempt API access with an invalid/expired/inactive API key. Inspect server logs.
* **Prerequisites:** API is running.
* **Expected Secure Outcome:** Failed authentication attempts (e.g., invalid key, expired key, inactive key) are logged with relevant details (e.g., reason for failure, attempted key prefix if safe, request context like IP).
* **Verification Steps:**
    1.  Attempt to call `/api/v1/models` with an invalid API key.
    2.  Verify the API returns a 401 error.
    3.  Inspect server logs. Confirm a log entry (e.g., level WARNING) indicating the authentication failure, potentially including the `api_key_id` if the key existed but was invalid for other reasons (e.g. expired), or a generic message if the key was not found.
    4.  Confirm logs related to "API key is expired" or "Missing or invalid API key" are generated by `app.auth.dependencies`.

### **ZTA_CM_004**
* **ID:** ZTA_CM_004
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Verify logging of authorization failures (insufficient scope).
* **Exposure Point(s):** `RequiresScope` class in `app/auth/dependencies.py` (implicitly, as it raises `HTTPException` which `StructlogMiddleware` would log).
* **Test Method/Action:** Attempt API access to a scoped endpoint (e.g., `/api/v1/chat/completions`) with a valid key that lacks the required scope. Inspect server logs.
* **Prerequisites:** API is running. An API key exists with valid authentication but insufficient scope for the target endpoint.
* **Expected Secure Outcome:** Authorization failures are logged, including the API key identifier, requested endpoint, and the scope issue.
* **Verification Steps:**
    1.  Attempt to call `/api/v1/chat/completions` with a key that has, for example, only `models:embedding` scope.
    2.  Verify the API returns a 401 error with `{"detail": "Not Authorized"}`.
    3.  Inspect server logs. Confirm a log entry associated with the 401 response, including the `request_id`, `api_key_id`, and details of the request. While `RequiresScope` itself doesn't log explicitly before raising, the `StructlogMiddleware` will log the 401 event.

### **ZTA_CM_005**
* **ID:** ZTA_CM_005
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Verify logging of LLM usage for billing/audit.
* **Exposure Point(s):** `billing_worker` in `app/services/billing.py:13-23`, and how `billing_data` is populated in `app/routers/api_v1.py`.
* **Test Method/Action:** Make successful calls to `/api/v1/chat/completions` and `/api/v1/embeddings`. Inspect server logs for billing entries.
* **Prerequisites:** API is running.
* **Expected Secure Outcome:** LLM usage (model ID, user/key identifier, token counts) is logged for compliance and audit trails.
* **Verification Steps:**
    1.  Make a successful chat completion request and an embedding request.
    2.  Inspect server logs for entries with `event="billing"`.
    3.  Confirm these logs contain fields like `api_key_id`, `manager_id` (user UUID), `model` (model ID), `prompt_tokens`, `completion_tokens`, `total_tokens`.

### **ZTA_CM_006**
* **ID:** ZTA_CM_006
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Verify logging of LLM provider interaction details (metrics).
* **Exposure Point(s):** Provider backends, e.g., `app/providers/vertex_ai/vertexai.py:80-99-116` (for Vertex, captures model metrics and LLM interaction details) and similar logic in Bedrock.
* **Test Method/Action:** Make successful calls to models on different providers (Bedrock, Vertex AI). Inspect server logs.
* **Prerequisites:** API is running. Models from both Bedrock and Vertex AI are configured.
* **Expected Secure Outcome:** Key metrics from LLM provider interactions (e.g., latency, token counts if available from headers/response metadata) are logged.
* **Verification Steps:**
    1.  Make a call to a Vertex AI model and a Bedrock model.
    2.  For Vertex AI, check for logs like "vertex_ai_response_metrics" containing latency, token counts, etc. as per `vertexai.py` code.
    3.  For Bedrock, check for logs specific to Bedrock interactions that capture metrics like `latencyMs` or token usage if `BedrockBackend` logs them.

### **ZTA_CM_007**
* **ID:** ZTA_CM_007
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Verify absence of PII/sensitive data leakage in standard operational logs.
* **Exposure Point(s):** All logging components (`StructlogMiddleware`, `billing_worker`, provider backends).
* **Test Method/Action:** Send requests with mock sensitive data in prompts or other free-text fields. Review standard (INFO level) logs.
* **Prerequisites:** API is running.
* **Expected Secure Outcome:** Standard operational logs (INFO level) do not contain full user prompts, LLM responses, or other potentially sensitive PII from request bodies/responses. (Risk analysis notes "Critical Gaps: Missing PIIFilteringProcessor"). This test verifies current state.
* **Verification Steps:**
    1.  Send a chat request with body: `{"model": "some_model", "messages": [{"role": "user", "content": "My secret is 12345 and my SSN is MOCK_SSN"}]}`.
    2.  Inspect server logs at INFO level. Confirm that "My secret is 12345 and my SSN is MOCK_SSN" is NOT present in these logs.
    3.  The `request_id`, `api_key_id`, model used, and token counts may be logged, but not the sensitive content itself.

### **ZTA_CM_008**
* **ID:** ZTA_CM_008
* **Category Ref:** Continuous Monitoring Verification
* **Description:** Assess log tamper-evidence and central aggregation (Conceptual).
* **Exposure Point(s):** Logging infrastructure and deployment practices.
* **Test Method/Action:** This is a process/infrastructure review, not a direct API test. Review how logs are collected, stored, and secured in the target deployment environment.
* **Prerequisites:** Understanding of the production logging pipeline.
* **Expected Secure Outcome:** Logs are centrally aggregated to a secure logging system (e.g., SIEM, dedicated log management platform). The system should have measures to ensure log integrity (e.g., write-once storage, access controls, audit trails for log access). (Risk analysis mentions "no external SIEM integration" as a gap).
* **Verification Steps:**
    1.  Discuss with operations/security team about log aggregation and protection strategy.
    2.  Verify if logs are shipped to a central system.
    3.  Inquire about access controls and integrity measures for the central logging system.

---

## Enhanced Test Cases: Advanced Continuous Monitoring

### 1. Real-Time Security Event Correlation

* **ID:** ZTA_CM_009
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test real-time correlation of security events across multiple sources with automated threat detection and response.
    * **Exposure Point(s):** Security event correlation engines, real-time analysis systems, automated response mechanisms.
    * **Test Method/Action:**
        1. Test correlation of authentication failures across multiple API keys
        2. Validate detection of coordinated attack patterns in real-time
        3. Test automated threat scoring based on event correlation
        4. Validate integration with threat intelligence feeds for context
        5. Test automated response triggers based on correlation results
    * **Prerequisites:** Event correlation platform, real-time analytics, threat intelligence integration, automated response systems.
    * **Expected Secure Outcome:** Related security events correlated in real-time. Attack patterns detected automatically. Threat intelligence enhances correlation accuracy.
    * **Verification Steps:** Test correlation accuracy, validate pattern detection, verify threat intelligence integration, check automated response effectiveness.

### 2. Behavioral Analytics and Anomaly Detection

* **ID:** ZTA_CM_010
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test behavioral analytics for user and API key usage patterns with machine learning-based anomaly detection.
    * **Exposure Point(s):** Behavioral analytics engines, ML-based detection systems, usage pattern analysis.
    * **Test Method/Action:**
        1. Test establishment of baseline behavioral patterns for API keys
        2. Validate anomaly detection for unusual usage patterns
        3. Test machine learning model accuracy and false positive rates
        4. Validate behavioral scoring and risk assessment
        5. Test adaptive learning and model improvement over time
    * **Prerequisites:** Behavioral analytics platform, ML infrastructure, baseline establishment systems, risk scoring capabilities.
    * **Expected Secure Outcome:** Baseline behaviors established accurately. Anomalies detected with high precision. Risk scores reflect actual threat levels.
    * **Verification Steps:** Test baseline accuracy, validate anomaly detection precision, verify risk scoring, check adaptive learning effectiveness.

### 3. Advanced PII Detection and Redaction

* **ID:** ZTA_CM_011
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test advanced PII detection and redaction in logs using ML-based pattern recognition and context-aware filtering.
    * **Exposure Point(s):** PII detection systems, ML-based pattern recognition, context-aware filtering, redaction mechanisms.
    * **Test Method/Action:**
        1. Test ML-based detection of PII patterns in request data
        2. Validate context-aware filtering that preserves operational data
        3. Test real-time redaction of sensitive information in logs
        4. Validate accuracy of PII detection across different data formats
        5. Test audit trail for redaction actions and policy compliance
    * **Prerequisites:** ML-based PII detection, context-aware filtering systems, real-time redaction capabilities, audit frameworks.
    * **Expected Secure Outcome:** PII detected with 95%+ accuracy. Sensitive data redacted while preserving operational visibility. Audit trails complete and compliant.
    * **Verification Steps:** Test detection accuracy, validate redaction effectiveness, verify operational data preservation, check audit completeness.

### 4. Threat Intelligence Integration

* **ID:** ZTA_CM_012
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test integration with external threat intelligence feeds for enhanced monitoring and contextual threat analysis.
    * **Exposure Point(s):** Threat intelligence platforms, external feed integration, contextual analysis systems.
    * **Test Method/Action:**
        1. Test integration with multiple threat intelligence feeds
        2. Validate enrichment of security events with threat context
        3. Test automated IOC (Indicators of Compromise) matching
        4. Validate threat landscape awareness and adaptive monitoring
        5. Test intelligence-driven alerting and prioritization
    * **Prerequisites:** Threat intelligence platform, multiple intelligence feeds, IOC matching systems, adaptive monitoring capabilities.
    * **Expected Secure Outcome:** Intelligence feeds properly integrated. Security events enriched with threat context. IOC matching accurate and timely.
    * **Verification Steps:** Test feed integration, validate context enrichment, verify IOC matching accuracy, check adaptive monitoring effectiveness.

### 5. Compliance Monitoring and Reporting

* **ID:** ZTA_CM_013
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test automated compliance monitoring with regulatory framework adherence and automated reporting capabilities.
    * **Exposure Point(s):** Compliance monitoring systems, regulatory framework tracking, automated reporting engines.
    * **Test Method/Action:**
        1. Test monitoring for compliance with FISMA, SOX, GDPR requirements
        2. Validate automated detection of compliance violations
        3. Test generation of compliance reports and audit evidence
        4. Validate real-time compliance dashboards and alerting
        5. Test regulatory change tracking and implementation monitoring
    * **Prerequisites:** Compliance monitoring platform, regulatory frameworks, automated reporting systems, dashboard capabilities.
    * **Expected Secure Outcome:** Compliance violations detected automatically. Reports generated accurately and timely. Regulatory changes tracked and implemented.
    * **Verification Steps:** Test violation detection, validate report accuracy, verify dashboard functionality, check regulatory tracking effectiveness.

### 6. Distributed Tracing and Observability

* **ID:** ZTA_CM_014
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test distributed tracing across all system components with comprehensive observability and performance monitoring.
    * **Exposure Point(s):** Distributed tracing systems, observability platforms, performance monitoring, trace correlation.
    * **Test Method/Action:**
        1. Test end-to-end tracing from client request to LLM provider response
        2. Validate trace correlation across microservices and components
        3. Test performance monitoring and bottleneck identification
        4. Validate error propagation tracking and root cause analysis
        5. Test integration with monitoring and alerting systems
    * **Prerequisites:** Distributed tracing platform (Jaeger/Zipkin), observability tools, performance monitoring systems.
    * **Expected Secure Outcome:** Complete trace visibility across all components. Performance bottlenecks identified accurately. Root cause analysis simplified.
    * **Verification Steps:** Test trace completeness, validate correlation accuracy, verify performance monitoring, check root cause analysis effectiveness.

### 7. Security Orchestration and Automated Response

* **ID:** ZTA_CM_015
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test security orchestration platform integration with automated incident response and threat mitigation capabilities.
    * **Exposure Point(s):** Security orchestration platforms, automated response systems, incident management, threat mitigation.
    * **Test Method/Action:**
        1. Test automated incident creation and escalation workflows
        2. Validate orchestrated response actions across security tools
        3. Test threat containment and mitigation automation
        4. Validate playbook execution and decision making
        5. Test integration with incident management and communication systems
    * **Prerequisites:** Security orchestration platform (SOAR), automated response capabilities, incident management systems.
    * **Expected Secure Outcome:** Incidents handled automatically with appropriate escalation. Response actions coordinated effectively. Threats contained rapidly.
    * **Verification Steps:** Test automation accuracy, validate response coordination, verify containment effectiveness, check escalation procedures.

### 8. Continuous Security Posture Assessment

* **ID:** ZTA_CM_016
    * **Category Ref:** Continuous Monitoring Verification
    * **Description:** Test continuous assessment of security posture with automated vulnerability detection and risk measurement.
    * **Exposure Point(s):** Security posture assessment tools, vulnerability detection systems, risk measurement platforms.
    * **Test Method/Action:**
        1. Test continuous assessment of security control effectiveness
        2. Validate automated vulnerability detection in runtime environments
        3. Test risk scoring and posture measurement accuracy
        4. Validate trend analysis and posture improvement tracking
        5. Test integration with security metrics and KPI dashboards
    * **Prerequisites:** Security posture assessment platform, vulnerability detection tools, risk measurement systems.
    * **Expected Secure Outcome:** Security posture continuously assessed and measured. Vulnerabilities detected in real-time. Risk trends tracked accurately.
    * **Verification Steps:** Test assessment accuracy, validate vulnerability detection, verify risk measurement, check trend analysis effectiveness.

---