'''
This is an intermediate format representing an abstract chat request.
It's purpose it to insulate from changes to the OpenAI API and prevent
re-writing conversion code.

Other provides should provide adapters for converting to and from this schema
and should never "know" about other provider formats (including OpenAI's).

OpenAI requests will be converted into this form and wll concrete representations
should convert from this.

                  |-> Bedrock
OpenAI -> Core -> |-> Vertex
                  |-> Others
'''
from datetime import datetime
from typing import Literal, Annotated, Optional, List, Any, Sequence, Union
from pydantic import BaseModel, Field, BeforeValidator

def convert_str(value: Any) -> Any:  
    if isinstance(value, str):  
        return [{"type":"text", "text": value}]
    else:
        return value


# ----> Chat Request Model <---- #
class TextPart(BaseModel):
    type: Literal["text"] = "text"
    text: str

class ImagePart(BaseModel):
    type: Literal["image", "image_url"] = "image"
    file_type: Literal['jpeg', 'png', 'gif', 'webp']
    bytes_: bytes = Field(..., alias="bytes")

class FilePart(BaseModel):
    type: Literal["file"] = "file"
    mime_type: str
    name: Optional[str] = Field(default=None)
    bytes_: bytes = Field(..., alias="bytes")

ContentPart = Annotated[TextPart | ImagePart | FilePart, Field(discriminator="type")]

class FunctionCall(BaseModel):
    name: Optional[str] = None
    arguments: str # raw JSON string


class ToolCall(BaseModel):
    """Represents a tool call from the assistant."""
    index: Optional[int] = Field(None, description="Tool call index useed in streaming")
    id: Optional[str] = Field(default=None, description="Unique identifier for the tool call")
    type: Literal["function"] = "function"
    function: FunctionCall = Field(
        ..., description="Function call details, including 'name' and 'arguments'"
    )

class UserMessage(BaseModel):
    role: Literal["user"] = "user"  
    content: Annotated[Sequence[ContentPart], BeforeValidator(convert_str)]

class AssistantMessage(BaseModel):
    role: Literal["assistant"] = "assistant"  
    content: Annotated[Sequence[ContentPart], BeforeValidator(convert_str)]
    tool_calls: Optional[List[ToolCall]] = Field(default=None, description="List of tool calls generated by the model, such as function calls.")

class ToolMessage(BaseModel):
    role: Literal["tool"] = "tool"
    tool_call_id: str = Field(..., description="The ID of the tool call associated with this message")
    content: Union[str, Sequence[TextPart]] = Field(description="The content of the message from the tool. Can be a string, a list of text parts")


class SystemMessage(BaseModel):
    role: Literal['system'] = "system"
    content: Annotated[Sequence[TextPart], BeforeValidator(convert_str)]

Message = Annotated[UserMessage | AssistantMessage | SystemMessage | ToolMessage, Field(discriminator="role")]

class FunctionParameters(BaseModel):
    type: Literal["object"] = "object"
    properties: dict = Field(
        ..., description="Parameter properties as JSON Schema objects"
    )
    required: Optional[List[str]] = None

class FunctionObject(BaseModel):
    name: str = Field(..., description="The name of the function")
    description: Optional[str] = Field(
        None, description="A description of what the function does"
    )
    parameters: FunctionParameters = Field(
        ..., description="Parameters the function accepts, as a JSON Schema object"
    )

class ToolDefinition(BaseModel):
    """Describes a tool (function) the model may call"""
    type: Literal["function"] = "function"
    function: FunctionObject

class ChatRequest(BaseModel):
    model: str
    messages: Sequence[Message]
    tools: Optional[Sequence[ToolDefinition]] = None
    tool_choice: Optional[Union[str, dict]] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    stop: Optional[List[str]] = None



# ----> Chat Response Model <---- #
# This is almost identical to the OpenAI
# repsonse model. Keeping it seperate
# to maintain an layer of abstraction
class Response(BaseModel):
    content: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    finish_reason: Optional[Literal["stop", "length", "tool_calls", "content_filter"]] = None

    
class CompletionUsage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatRepsonse(BaseModel):
    created: datetime
    model: str
    choices: List[Response]
    usage: CompletionUsage


# ----> Chat Response Model <---- #

class StreamResponseDelta(BaseModel):
    content: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    refusal: Optional[str] = None
    role: Optional[str] = None

class StreamResponseChoice(BaseModel):
    delta: Optional[StreamResponseDelta] = None
    finish_reason: Optional[str] = None
    index: int
class StreamResponse(BaseModel):
    id: str
    choices: List[StreamResponseChoice]
    model: str
    created: datetime
    object: Literal["chat.completion.chunk"] = "chat.completion.chunk"
    service_tier: Optional[str] = None
    usage: Optional[CompletionUsage] = None